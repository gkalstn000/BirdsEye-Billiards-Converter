{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import tensorflow.compat.v1 as tf\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "\n",
    "#This is needed since the code is stored in the object_detection    folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "#Detection using tensorflow inside write_video function\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "#import object_detection.cut_obj as co\n",
    "import cordinate.Billiards_Detect_test as bd\n",
    "import cordinate.point_order as po\n",
    "import trans.imgwarp2 as iw\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                    'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                    'detection_classes', 'detection_masks'\n",
    "                    ]:\n",
    "              \n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          \n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "        \n",
    "        \n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "              # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "              # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict\n",
    "\n",
    "def class_cordinate(output_dict) :\n",
    "    num_detections = output_dict['detection_classes']\n",
    "    detection_boxes = output_dict['detection_boxes']\n",
    "    detection_scores = output_dict['detection_scores']\n",
    "    detection_classes = output_dict['detection_classes']\n",
    "\n",
    "    class_1_max_score = 0\n",
    "    class_2_max_score = 0\n",
    "    class_3_max_score = 0\n",
    "    class_4_max_score = 0\n",
    "    \n",
    "    class_1_ind = 0\n",
    "    class_2_ind = 0\n",
    "    class_3_ind = 0\n",
    "    class_4_ind = 0\n",
    "\n",
    "\n",
    "    for i in range(len(detection_classes)) :\n",
    "        if detection_classes[i] == 1 :\n",
    "            if class_1_max_score < detection_scores[i]:\n",
    "                class_1_max_score = detection_scores[i]\n",
    "                class_1_ind = i\n",
    "        elif detection_classes[i] == 2 :\n",
    "            if class_2_max_score < detection_scores[i]:\n",
    "                class_2_max_score = detection_scores[i]\n",
    "                class_2_ind = i\n",
    "        elif detection_classes[i] == 3 :\n",
    "            if class_3_max_score < detection_scores[i]:\n",
    "                class_3_max_score = detection_scores[i]\n",
    "                class_3_ind = i\n",
    "        else :\n",
    "            if class_4_max_score < detection_scores[i]:\n",
    "                class_4_max_score = detection_scores[i]\n",
    "                class_4_ind = i\n",
    "\n",
    "    result = {'table' : detection_boxes[class_2_ind],\n",
    "             'red_ball' : detection_boxes[class_1_ind],\n",
    "             'white_ball' : detection_boxes[class_3_ind],\n",
    "             'yellow_ball' : detection_boxes[class_4_ind]}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def img_cut(image_path) : \n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((845,526))\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "    cordinates = class_cordinate(output_dict)\n",
    "    img_height, img_width, img_channel = image_np.shape\n",
    "    absolute_coord = []\n",
    "    \n",
    "    for i in cordinates.keys() :\n",
    "        ymin, xmin, ymax, xmax = cordinates[i]\n",
    "        x_up = int(xmin*img_width)\n",
    "        y_up = int(ymin*img_height)\n",
    "        x_down = int(xmax*img_width)\n",
    "        y_down = int(ymax*img_height)\n",
    "        absolute_coord.append([x_up,y_up,x_down,y_down])\n",
    "    \n",
    "    bounding_box_img = []\n",
    "    c = absolute_coord[0]\n",
    "    bounding_box_img = image_np[c[1]:c[3], c[0]:c[2],:]\n",
    "    \n",
    "    \n",
    "    red = [(absolute_coord[1][0]+absolute_coord[1][2])//2 - c[0], (absolute_coord[1][1]+absolute_coord[1][3])//2 - c[1]]\n",
    "    white = [(absolute_coord[2][0]+absolute_coord[2][2])//2 - c[0], (absolute_coord[2][1]+absolute_coord[2][3])//2 - c[1]]\n",
    "    yellow = [(absolute_coord[3][0]+absolute_coord[3][2])//2 - c[0], (absolute_coord[3][1]+absolute_coord[3][3])//2 - c[1]]\n",
    "    \n",
    "    points = [white, red, yellow]\n",
    "    \n",
    "    result_dict = {'image_np' : bounding_box_img,\n",
    "                   'points' : points}\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from faster_rcnn_inception\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = './test_video_result/video_result.avi'\n",
    "codec = cv2.VideoWriter_fourcc('W', 'M', 'V', '2')\n",
    "cap = cv2.VideoCapture('./object_detection/test_video/video2.mp4')\n",
    "framerate = round(cap.get(5),2)\n",
    "w = int(cap.get(3))\n",
    "h = int(cap.get(4))\n",
    "resolution = (w, h)\n",
    "\n",
    "VideoFileOutput = cv2.VideoWriter(filename, codec, framerate, resolution)    \n",
    "\n",
    "################################\n",
    "# # Model preparation \n",
    "\n",
    "# ## Variables\n",
    "# \n",
    "# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "# \n",
    "\n",
    "\n",
    "# What model to download.\n",
    "PATH_TO_FROZEN_GRAPH = './object_detection/fine_tuned_model/frozen_inference_graph.pb'\n",
    "PATH_TO_LABEL_MAP = './object_detection/label_map.pbtxt'\n",
    "NUM_CLASSES = 4\n",
    "MODEL_NAME = 'faster_rcnn_inception'\n",
    "print(\"loading model from \" + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graphs\n",
      "tempo build graph = 1.0492160320281982\n"
     ]
    }
   ],
   "source": [
    "# ## Load a (frozen) Tensorflow model into memory.\n",
    "\n",
    "time_graph = time.time()\n",
    "print('loading graphs')\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "print(\"tempo build graph = \" + str(time.time() - time_graph))\n",
    "\n",
    "# ## Loading label map\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABEL_MAP)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frame number: 1.0\n",
      "time to capture video frame = 0.010015249252319336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkalstn/capstone/cordinate/point_order.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "  angle = np.arccos(np.dot(A, B) / (len_A*len_B))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to write a frame in video file = 2.193450927734375e-05\n",
      "total time in the loop = 4.726308822631836\n",
      "processing frame number: 2.0\n",
      "time to capture video frame = 0.004635334014892578\n",
      "time to write a frame in video file = 9.059906005859375e-06\n",
      "total time in the loop = 4.685392141342163\n",
      "processing frame number: 3.0\n",
      "time to capture video frame = 0.007575035095214844\n",
      "time to write a frame in video file = 1.5974044799804688e-05\n",
      "total time in the loop = 5.234823703765869\n",
      "processing frame number: 4.0\n",
      "time to capture video frame = 0.0051419734954833984\n",
      "time to write a frame in video file = 2.193450927734375e-05\n",
      "total time in the loop = 5.080516815185547\n",
      "processing frame number: 5.0\n",
      "time to capture video frame = 0.004441738128662109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkalstn/capstone/cordinate/point_order.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  angle = np.arccos(np.dot(A, B) / (len_A*len_B))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to write a frame in video file = 8.821487426757812e-06\n",
      "total time in the loop = 5.16704797744751\n",
      "processing frame number: 6.0\n",
      "time to capture video frame = 0.008157968521118164\n",
      "time to write a frame in video file = 8.344650268554688e-06\n",
      "total time in the loop = 5.555602073669434\n",
      "processing frame number: 7.0\n",
      "time to capture video frame = 0.004867076873779297\n",
      "time to write a frame in video file = 7.867813110351562e-06\n",
      "total time in the loop = 5.293557167053223\n",
      "processing frame number: 8.0\n",
      "time to capture video frame = 0.004724025726318359\n",
      "time to write a frame in video file = 7.152557373046875e-06\n",
      "total time in the loop = 5.09607720375061\n",
      "processing frame number: 9.0\n",
      "time to capture video frame = 0.004703044891357422\n",
      "time to write a frame in video file = 3.409385681152344e-05\n",
      "total time in the loop = 5.285001993179321\n",
      "processing frame number: 10.0\n",
      "time to capture video frame = 0.004875898361206055\n",
      "time to write a frame in video file = 8.106231689453125e-06\n",
      "total time in the loop = 5.716361999511719\n",
      "processing frame number: 11.0\n",
      "time to capture video frame = 0.0047490596771240234\n",
      "time to write a frame in video file = 7.867813110351562e-06\n",
      "total time in the loop = 5.36853289604187\n",
      "processing frame number: 12.0\n",
      "time to capture video frame = 0.004708290100097656\n",
      "time to write a frame in video file = 7.152557373046875e-06\n",
      "total time in the loop = 4.924917936325073\n",
      "processing frame number: 13.0\n",
      "time to capture video frame = 0.004876852035522461\n",
      "time to write a frame in video file = 7.867813110351562e-06\n",
      "total time in the loop = 5.741580009460449\n",
      "processing frame number: 14.0\n",
      "time to capture video frame = 0.0049800872802734375\n",
      "time to write a frame in video file = 8.106231689453125e-06\n",
      "total time in the loop = 5.3337578773498535\n",
      "processing frame number: 15.0\n",
      "time to capture video frame = 0.004530906677246094\n",
      "time to write a frame in video file = 9.059906005859375e-06\n",
      "total time in the loop = 4.73974609375\n",
      "processing frame number: 16.0\n",
      "time to capture video frame = 0.004639148712158203\n",
      "time to write a frame in video file = 3.0040740966796875e-05\n",
      "total time in the loop = 5.907847881317139\n",
      "processing frame number: 17.0\n",
      "time to capture video frame = 0.005293846130371094\n",
      "time to write a frame in video file = 7.867813110351562e-06\n",
      "total time in the loop = 5.912635087966919\n",
      "processing frame number: 18.0\n",
      "time to capture video frame = 0.004586935043334961\n",
      "time to write a frame in video file = 1.4066696166992188e-05\n",
      "total time in the loop = 5.026557922363281\n",
      "processing frame number: 19.0\n",
      "time to capture video frame = 0.007147073745727539\n",
      "time to write a frame in video file = 1.4066696166992188e-05\n",
      "total time in the loop = 4.927610158920288\n",
      "processing frame number: 20.0\n",
      "time to capture video frame = 0.004855155944824219\n",
      "time to write a frame in video file = 1.2874603271484375e-05\n",
      "total time in the loop = 5.148376941680908\n",
      "processing frame number: 21.0\n",
      "time to capture video frame = 0.005251169204711914\n",
      "time to write a frame in video file = 1.2636184692382812e-05\n",
      "total time in the loop = 4.978510141372681\n",
      "processing frame number: 22.0\n",
      "time to capture video frame = 0.0055081844329833984\n",
      "time to write a frame in video file = 8.821487426757812e-06\n",
      "total time in the loop = 5.6459801197052\n",
      "processing frame number: 23.0\n",
      "time to capture video frame = 0.006224155426025391\n",
      "time to write a frame in video file = 9.298324584960938e-06\n",
      "total time in the loop = 5.405779123306274\n",
      "processing frame number: 24.0\n",
      "time to capture video frame = 0.009956121444702148\n",
      "time to write a frame in video file = 7.867813110351562e-06\n",
      "total time in the loop = 5.459084987640381\n",
      "processing frame number: 25.0\n",
      "time to capture video frame = 0.008307933807373047\n",
      "time to write a frame in video file = 8.821487426757812e-06\n",
      "total time in the loop = 5.416908025741577\n",
      "processing frame number: 26.0\n",
      "time to capture video frame = 0.004672050476074219\n",
      "time to write a frame in video file = 6.9141387939453125e-06\n",
      "total time in the loop = 4.645617723464966\n",
      "processing frame number: 27.0\n",
      "time to capture video frame = 0.009557962417602539\n",
      "time to write a frame in video file = 6.9141387939453125e-06\n",
      "total time in the loop = 4.539060831069946\n",
      "processing frame number: 28.0\n",
      "time to capture video frame = 0.006216287612915039\n",
      "time to write a frame in video file = 1.5020370483398438e-05\n",
      "total time in the loop = 5.186414003372192\n",
      "processing frame number: 29.0\n",
      "time to capture video frame = 0.009643077850341797\n",
      "time to write a frame in video file = 2.288818359375e-05\n",
      "total time in the loop = 4.795372009277344\n",
      "processing frame number: 30.0\n",
      "time to capture video frame = 0.005491018295288086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-520913ef531c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#=========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference_for_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mcordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_cordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3f69ae7fcc5f>\u001b[0m in \u001b[0;36mrun_inference_for_single_image\u001b[0;34m(image, graph)\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             output_dict = sess.run(tensor_dict,\n\u001b[0;32m---> 39\u001b[0;31m                                  feed_dict={image_tensor: np.expand_dims(image, 0)})\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0;31m# all outputs are float32 numpy arrays, so convert types as appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    with detection_graph.as_default():\n",
    "        while (cap.isOpened()):\n",
    "            time_loop = time.time()\n",
    "            print('processing frame number: ' + str(cap.get(1)))\n",
    "            time_captureframe = time.time()\n",
    "            ret, image_np = cap.read()\n",
    "            print(\"time to capture video frame = \" + str(time.time() - time_captureframe))\n",
    "            if (ret != True):\n",
    "                break\n",
    "                \n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            #image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            \n",
    "            #=========================\n",
    "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "            cordinates = class_cordinate(output_dict)\n",
    "            img_height, img_width, img_channel = image_np.shape\n",
    "            absolute_coord = []\n",
    "            for i in cordinates.keys() :\n",
    "                ymin, xmin, ymax, xmax = cordinates[i]\n",
    "                x_up = int(xmin*img_width)\n",
    "                y_up = int(ymin*img_height)\n",
    "                x_down = int(xmax*img_width)\n",
    "                y_down = int(ymax*img_height)\n",
    "                absolute_coord.append([x_up,y_up,x_down,y_down])\n",
    "            bounding_box_img = []\n",
    "            c = absolute_coord[0]\n",
    "            bounding_box_img = image_np[c[1]:c[3], c[0]:c[2],:]\n",
    "            red = [(absolute_coord[1][0]+absolute_coord[1][2])//2 - c[0], (absolute_coord[1][1]+absolute_coord[1][3])//2 - c[1]]\n",
    "            white = [(absolute_coord[2][0]+absolute_coord[2][2])//2 - c[0], (absolute_coord[2][1]+absolute_coord[2][3])//2 - c[1]]\n",
    "            yellow = [(absolute_coord[3][0]+absolute_coord[3][2])//2 - c[0], (absolute_coord[3][1]+absolute_coord[3][3])//2 - c[1]]\n",
    "    \n",
    "            points = [white, red, yellow]\n",
    "#            image_np = bounding_box_img\n",
    "    \n",
    "\n",
    "            #=========================\n",
    "            \n",
    "            result = np.array(bd.Detecting(bounding_box_img))\n",
    "            result = po.point_order(result)\n",
    "            \n",
    "            all_points = [(result[0][0], result[0][1]),\n",
    "                         (result[1][0], result[1][1]),\n",
    "                         (result[2][0], result[2][1]),\n",
    "                         (result[3][0], result[3][1])]\n",
    "\n",
    "            all_points.append((points[0][0], points[0][1]))\n",
    "            all_points.append((points[1][0], points[1][1]))\n",
    "            all_points.append((points[2][0], points[2][1]))\n",
    "            \n",
    "            ani = iw.warp(all_points)\n",
    "                \n",
    "\n",
    "\n",
    "            time_writeframe = time.time()\n",
    "            VideoFileOutput.write(ani)\n",
    "            print(\"time to write a frame in video file = \" + str(time.time() - time_writeframe))\n",
    "\n",
    "            print(\"total time in the loop = \" + str(time.time() - time_loop))\n",
    "\n",
    "cap.release()\n",
    "VideoFileOutput.release()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
