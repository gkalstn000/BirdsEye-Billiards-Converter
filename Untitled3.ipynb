{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gkalstn/capstone\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from collections import defaultdict\n",
    "#from io import StringIO\n",
    "#from matplotlib import pyplot as plt\n",
    "#from PIL import Image\n",
    "#from distutils.version import LooseVersion, StrictVersion\n",
    "from distutils.version import StrictVersion\n",
    "\n",
    "import cv2\n",
    "#from imutils import paths\n",
    "import tensorflow.compat.v1 as tf\n",
    "import time\n",
    "#import re\n",
    "import sys\n",
    "\n",
    "#This is needed since the code is stored in the object_detection    folder.\n",
    "sys.path.append(\"..\")\n",
    "#from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "#Detection using tensorflow inside write_video function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                    'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                    'detection_classes', 'detection_masks'\n",
    "                    ]:\n",
    "              \n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "          \n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "        \n",
    "        \n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "              # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "              # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict\n",
    "\n",
    "def class_cordinate(output_dict) :\n",
    "    detection_boxes = output_dict['detection_boxes']\n",
    "    detection_scores = output_dict['detection_scores']\n",
    "    detection_classes = output_dict['detection_classes']\n",
    "\n",
    "    class_1_max_score = 0\n",
    "    class_2_max_score = 0\n",
    "    class_3_max_score = 0\n",
    "    class_4_max_score = 0\n",
    "    \n",
    "    class_1_ind = 0\n",
    "    class_2_ind = 0\n",
    "    class_3_ind = 0\n",
    "    class_4_ind = 0\n",
    "\n",
    "\n",
    "    for i in range(len(detection_classes)) :\n",
    "        if detection_classes[i] == 1 :\n",
    "            if class_1_max_score < detection_scores[i]:\n",
    "                class_1_max_score = detection_scores[i]\n",
    "                class_1_ind = i\n",
    "        elif detection_classes[i] == 2 :\n",
    "            if class_2_max_score < detection_scores[i]:\n",
    "                class_2_max_score = detection_scores[i]\n",
    "                class_2_ind = i\n",
    "        elif detection_classes[i] == 3 :\n",
    "            if class_3_max_score < detection_scores[i]:\n",
    "                class_3_max_score = detection_scores[i]\n",
    "                class_3_ind = i\n",
    "        else :\n",
    "            if class_4_max_score < detection_scores[i]:\n",
    "                class_4_max_score = detection_scores[i]\n",
    "                class_4_ind = i\n",
    "\n",
    "    result = {'table' : detection_boxes[class_2_ind],\n",
    "             'red_ball' : detection_boxes[class_1_ind],\n",
    "             'white_ball' : detection_boxes[class_3_ind],\n",
    "             'yellow_ball' : detection_boxes[class_4_ind]}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def img_cut(image_np) : \n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "    cordinates = class_cordinate(output_dict)\n",
    "    img_height, img_width, img_channel = image_np.shape\n",
    "    absolute_coord = []\n",
    "    \n",
    "    for i in cordinates.keys() :\n",
    "        ymin, xmin, ymax, xmax = cordinates[i]\n",
    "        x_up = int(xmin*img_width)\n",
    "        y_up = int(ymin*img_height)\n",
    "        x_down = int(xmax*img_width)\n",
    "        y_down = int(ymax*img_height)\n",
    "        absolute_coord.append([x_up,y_up,x_down,y_down])\n",
    "    \n",
    "    bounding_box_img = []\n",
    "    c = absolute_coord[0]\n",
    "    bounding_box_img = image_np[c[1]:c[3], c[0]:c[2],:]\n",
    "    \n",
    "    \n",
    "    red = [(absolute_coord[1][0]+absolute_coord[1][2])//2 - c[0], (absolute_coord[1][1]+absolute_coord[1][3])//2 - c[1]]\n",
    "    white = [(absolute_coord[2][0]+absolute_coord[2][2])//2 - c[0], (absolute_coord[2][1]+absolute_coord[2][3])//2 - c[1]]\n",
    "    yellow = [(absolute_coord[3][0]+absolute_coord[3][2])//2 - c[0], (absolute_coord[3][1]+absolute_coord[3][3])//2 - c[1]]\n",
    "    \n",
    "        \n",
    "    points = [white, red, yellow]\n",
    "    \n",
    "    return bounding_box_img, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import cordinate.Billiards_Detect_test as bd\n",
    "import cordinate.point_order as po\n",
    "import trans.imgwarp as iw\n",
    "import trans.show_result as sr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from faster_rcnn_inception\n",
      "loading graphs\n",
      "tempo build graph = 1.2850301265716553\n",
      "processing frame number: 0.0\n",
      "time to capture video frame = 0.030549049377441406\n",
      "당구대 [(511, 226), (376, 71), (511, 226), (376, 71)]\n",
      "당구공 [(20, 106), (90, 296), (20, 106)]\n",
      "time to image to grapic in a frame = 4.894028186798096\n",
      "time to write a frame in video file = 1.0967254638671875e-05\n",
      "total time in the loop = 4.937238931655884\n",
      "processing frame number: 1.0\n",
      "time to capture video frame = 0.023947954177856445\n",
      "당구대 [(511, 226), (376, 70), (511, 226), (376, 70)]\n",
      "당구공 [(24, 111), (94, 301), (24, 111)]\n",
      "time to image to grapic in a frame = 4.9214699268341064\n",
      "time to write a frame in video file = 1.0967254638671875e-05\n",
      "total time in the loop = 4.954087257385254\n",
      "processing frame number: 2.0\n",
      "time to capture video frame = 0.025953054428100586\n",
      "당구대 [(511, 226), (376, 70), (511, 226), (376, 70)]\n",
      "당구공 [(26, 107), (95, 297), (26, 107)]\n",
      "time to image to grapic in a frame = 5.229222059249878\n",
      "time to write a frame in video file = 1.0967254638671875e-05\n",
      "total time in the loop = 5.262887954711914\n",
      "processing frame number: 3.0\n",
      "time to capture video frame = 0.02434682846069336\n",
      "당구대 [(511, 225), (387, 82), (511, 225), (387, 82)]\n",
      "당구공 [(52, 187), (97, 297), (27, 107)]\n",
      "time to image to grapic in a frame = 5.041305065155029\n",
      "time to write a frame in video file = 1.0967254638671875e-05\n",
      "total time in the loop = 5.073604106903076\n",
      "processing frame number: 4.0\n",
      "time to capture video frame = 0.030786752700805664\n",
      "당구대 [(511, 226), (376, 70), (511, 226), (376, 70)]\n",
      "당구공 [(55, 178), (99, 287), (29, 98)]\n",
      "time to image to grapic in a frame = 4.975118160247803\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 5.01295804977417\n",
      "processing frame number: 5.0\n",
      "time to capture video frame = 0.026121854782104492\n",
      "당구대 [(511, 225), (376, 70), (511, 225), (376, 70)]\n",
      "당구공 [(55, 177), (98, 286), (29, 98)]\n",
      "time to image to grapic in a frame = 4.731750011444092\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 4.7647950649261475\n",
      "processing frame number: 6.0\n",
      "time to capture video frame = 0.025120973587036133\n",
      "당구대 [(511, 225), (380, 74), (511, 225), (380, 74)]\n",
      "당구공 [(53, 186), (96, 296), (53, 186)]\n",
      "time to image to grapic in a frame = 4.6746251583099365\n",
      "time to write a frame in video file = 1.5974044799804688e-05\n",
      "total time in the loop = 4.715131759643555\n",
      "processing frame number: 7.0\n",
      "time to capture video frame = 0.026724815368652344\n",
      "당구대 [(512, 228), (375, 70), (512, 228), (375, 70)]\n",
      "당구공 [(46, 188), (89, 298), (46, 188)]\n",
      "time to image to grapic in a frame = 5.050502777099609\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 5.084681987762451\n",
      "processing frame number: 8.0\n",
      "time to capture video frame = 0.02465987205505371\n",
      "당구대 [(512, 228), (375, 70), (512, 228), (375, 70)]\n",
      "당구공 [(54, 185), (96, 296), (54, 185)]\n",
      "time to image to grapic in a frame = 5.202509880065918\n",
      "time to write a frame in video file = 1.1205673217773438e-05\n",
      "total time in the loop = 5.234506130218506\n",
      "processing frame number: 9.0\n",
      "time to capture video frame = 0.025171279907226562\n",
      "당구대 [(512, 226), (373, 67), (512, 226), (373, 67)]\n",
      "당구공 [(55, 187), (97, 298), (55, 187)]\n",
      "time to image to grapic in a frame = 5.3218138217926025\n",
      "time to write a frame in video file = 1.0967254638671875e-05\n",
      "total time in the loop = 5.355568170547485\n",
      "processing frame number: 10.0\n",
      "time to capture video frame = 0.025783061981201172\n",
      "당구대 [(512, 226), (375, 68), (512, 226), (375, 68)]\n",
      "당구공 [(54, 187), (97, 298), (54, 187)]\n",
      "time to image to grapic in a frame = 5.046793222427368\n",
      "time to write a frame in video file = 4.291534423828125e-05\n",
      "total time in the loop = 5.088343143463135\n",
      "processing frame number: 11.0\n",
      "time to capture video frame = 0.03648018836975098\n",
      "당구대 [(512, 227), (376, 71), (512, 227), (376, 71)]\n",
      "당구공 [(53, 191), (95, 302), (27, 113)]\n",
      "time to image to grapic in a frame = 5.154478073120117\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 5.199538946151733\n",
      "processing frame number: 12.0\n",
      "time to capture video frame = 0.023735761642456055\n",
      "당구대 [(512, 227), (374, 68), (512, 227), (374, 68)]\n",
      "당구공 [(55, 189), (96, 299), (27, 110)]\n",
      "time to image to grapic in a frame = 4.6811089515686035\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 4.7652270793914795\n",
      "processing frame number: 13.0\n",
      "time to capture video frame = 0.024964094161987305\n",
      "당구대 [(512, 225), (376, 68), (512, 225), (376, 68)]\n",
      "당구공 [(54, 190), (97, 301), (28, 111)]\n",
      "time to image to grapic in a frame = 4.955639123916626\n",
      "time to write a frame in video file = 1.0013580322265625e-05\n",
      "total time in the loop = 4.9877612590789795\n",
      "processing frame number: 14.0\n",
      "time to capture video frame = 0.024418115615844727\n",
      "당구대 [(512, 225), (376, 68), (512, 225), (376, 68)]\n",
      "당구공 [(55, 187), (97, 299), (26, 109)]\n",
      "time to image to grapic in a frame = 5.032754898071289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1989322b70ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mwrite_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1989322b70ce>\u001b[0m in \u001b[0;36mwrite_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time to image to grapic in a frame = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_graphiclize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./imgs/after_results.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def write_video():\n",
    "\n",
    "    filename = './test_video_result/video_result.avi'\n",
    "    codec = cv2.VideoWriter_fourcc('W', 'M', 'V', '2')\n",
    "    cap = cv2.VideoCapture('./test_video/video3.mp4')\n",
    "    framerate = round(cap.get(5),2)\n",
    "    w = int(cap.get(3))\n",
    "    h = int(cap.get(4))\n",
    "    resolution = (w, h)\n",
    "\n",
    "    VideoFileOutput = cv2.VideoWriter(filename, codec, framerate, resolution)    \n",
    "\n",
    "    ################################\n",
    "    # # Model preparation \n",
    "\n",
    "    # ## Variables\n",
    "    # \n",
    "    # Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "    # \n",
    "\n",
    "\n",
    "    # What model to download.\n",
    "    PATH_TO_FROZEN_GRAPH = './object_detection/fine_tuned_model/frozen_inference_graph.pb'\n",
    "    PATH_TO_LABEL_MAP = './object_detection/label_map.pbtxt'\n",
    "    NUM_CLASSES = 4\n",
    "    MODEL_NAME = 'faster_rcnn_inception'\n",
    "    print(\"loading model from \" + MODEL_NAME)\n",
    "\n",
    "\n",
    "    # ## Load a (frozen) Tensorflow model into memory.\n",
    "\n",
    "    time_graph = time.time()\n",
    "    print('loading graphs')\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    print(\"tempo build graph = \" + str(time.time() - time_graph))\n",
    "\n",
    "    # ## Loading label map\n",
    "\n",
    "    label_map = label_map_util.load_labelmap(PATH_TO_LABEL_MAP)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "    ################################\n",
    "\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        with detection_graph.as_default():\n",
    "            while (cap.isOpened()):\n",
    "                time_loop = time.time()\n",
    "                print('processing frame number: ' + str(cap.get(1)))\n",
    "                time_captureframe = time.time()\n",
    "                ret, image_np = cap.read()\n",
    "                cv2.imwrite(\"./imgs/before_results.png\", image_np)\n",
    "                print(\"time to capture video frame = \" + str(time.time() - time_captureframe))\n",
    "                if (ret != True):\n",
    "                    break\n",
    "                time_graphiclize = time.time()\n",
    "                # the array based representation of the image will be used later in order to prepare the\n",
    "                # result image with boxes and labels on it.\n",
    "                #image_np = load_image_into_numpy_array(image)\n",
    "                # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "                #==================================================\n",
    "                output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "                cordinates = class_cordinate(output_dict)\n",
    "                img_height, img_width, img_channel = image_np.shape\n",
    "                absolute_coord = []\n",
    "    \n",
    "                for i in cordinates.keys() :\n",
    "                    ymin, xmin, ymax, xmax = cordinates[i]\n",
    "                    x_up = int(xmin*img_width)\n",
    "                    y_up = int(ymin*img_height)\n",
    "                    x_down = int(xmax*img_width)\n",
    "                    y_down = int(ymax*img_height)\n",
    "                    absolute_coord.append([x_up,y_up,x_down,y_down])\n",
    "    \n",
    "                bounding_box_img = []\n",
    "                c = absolute_coord[0]\n",
    "                bounding_box_img = image_np[c[1]:c[3], c[0]:c[2],:]\n",
    "    \n",
    "    \n",
    "                red = [(absolute_coord[1][0]+absolute_coord[1][2])//2 - c[0], (absolute_coord[1][1]+absolute_coord[1][3])//2 - c[1]]\n",
    "                white = [(absolute_coord[2][0]+absolute_coord[2][2])//2 - c[0], (absolute_coord[2][1]+absolute_coord[2][3])//2 - c[1]]\n",
    "                yellow = [(absolute_coord[3][0]+absolute_coord[3][2])//2 - c[0], (absolute_coord[3][1]+absolute_coord[3][3])//2 - c[1]]\n",
    "    \n",
    "        \n",
    "                points = [white, red, yellow]\n",
    "                \n",
    "                \n",
    "                                \n",
    "                #=================================================\n",
    "                result = np.array(bd.Detecting(image_np))\n",
    "                result = po.point_order(result)\n",
    "                \n",
    "                all_points = [(result[0][0], result[0][1]),\n",
    "                             (result[1][0], result[1][1]),\n",
    "                             (result[2][0], result[2][1]),\n",
    "                             (result[3][0], result[3][1])]\n",
    "\n",
    "                all_points.append((points[0][0], points[0][1]))\n",
    "                all_points.append((points[1][0], points[1][1]))\n",
    "                all_points.append((points[2][0], points[2][1]))\n",
    "                print('당구대', all_points[:4])\n",
    "                print('당구공', all_points[4:])\n",
    "                imgg = iw.warp(all_points)\n",
    "            \n",
    "                \n",
    "                print(\"time to image to grapic in a frame = \" + str(time.time() - time_graphiclize))\n",
    "                cv2.imwrite(\"./imgs/after_results.png\", imgg)\n",
    "\n",
    "\n",
    "\n",
    "                time_writeframe = time.time()\n",
    "                VideoFileOutput.write(imgg)\n",
    "                print(\"time to write a frame in video file = \" + str(time.time() - time_writeframe))\n",
    "\n",
    "                print(\"total time in the loop = \" + str(time.time() - time_loop))\n",
    "\n",
    "    cap.release()\n",
    "    VideoFileOutput.release()\n",
    "    print('done')\n",
    "    \n",
    "write_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
